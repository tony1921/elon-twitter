#!/usr/bin/env python3
"""
Elon Musk Tweet é¢„æµ‹ç³»ç»Ÿ - MVP ç‰ˆæœ¬
ç®€åŒ–ç‰ˆï¼šå•æ–‡ä»¶è¿è¡Œï¼Œæ— éœ€æ•°æ®åº“

ä½¿ç”¨æ–¹æ³•ï¼š
1. pip install requests beautifulsoup4 playwright numpy scipy
2. python elon_predictor.py

é…ç½®ï¼šç¼–è¾‘ä¸‹é¢çš„ CONFIG éƒ¨åˆ†
"""

import requests
from bs4 import BeautifulSoup
import numpy as np
from scipy.stats import poisson, nbinom
from datetime import datetime, timedelta
import pytz
import time
import json
from pathlib import Path

# ============================================================
# CONFIG é…ç½®åŒºåŸŸ - è¯·æ ¹æ®éœ€è¦ä¿®æ”¹
# ============================================================

CONFIG = {
    # Polymarket å¸‚åœºä¿¡æ¯ï¼ˆéœ€è¦æ‰‹åŠ¨å¡«å†™ï¼‰
    'market_url': 'https://polymarket.com/event/elon-musk-of-tweets-january-2-january-9',

    # æ—¶é—´çª—å£ï¼ˆæ‰‹åŠ¨å¡«å†™ï¼Œä¾‹å¦‚ï¼šJanuary 2, 2026 12:00 PM ETï¼‰
    'window_start_et': '2026-01-02 12:00 PM',  # æ ¼å¼ï¼šYYYY-MM-DD HH:MM AM/PM
    'window_end_et': '2026-01-09 12:00 PM',

    # XTracker URL
    'xtracker_url': 'https://xtracker.polymarket.com/user/elonmusk',

    # æŠ“å–é—´éš”ï¼ˆç§’ï¼‰
    'scrape_interval_seconds': 120,  # é»˜è®¤ 2 åˆ†é’Ÿ

    # æ¨¡å‹ç±»å‹
    'model_type': 'poisson',  # 'poisson' æˆ– 'neg_binom'

    # è¾“å‡ºæ–‡ä»¶
    'output_file': 'predictions.json',

    # æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
    'verbose': True,
}

# ============================================================
# æ ¸å¿ƒä»£ç  - ä¸éœ€è¦ä¿®æ”¹
# ============================================================


class ElonTweetPredictor:
    """Elon Musk æ¨æ–‡æ•°é‡é¢„æµ‹å™¨"""

    def __init__(self, config: dict):
        self.config = config
        self.historical_data = self._load_historical_data()
        self.predictions_history = []

    def _load_historical_data(self) -> dict:
        """åŠ è½½å†å²ç»Ÿè®¡æ•°æ®ï¼ˆç®€åŒ–ç‰ˆï¼šä½¿ç”¨å›ºå®šå…ˆéªŒï¼‰"""
        return {
            'avg_total_tweets': 400,  # åŸºäºå…ˆéªŒçŸ¥è¯†ï¼šElon å¹³å‡æ¯å‘¨çº¦400æ¡
            'avg_hourly_rate': 400 / (7 * 24),  # æ¯å°æ—¶çº¦ 2.38 æ¡
        }

    def parse_time_window(self) -> dict:
        """è§£ææ—¶é—´çª—å£"""
        et = pytz.timezone('America/New_York')
        utc = pytz.UTC

        # è§£æ ET æ—¶é—´
        start_et = et.localize(
            datetime.strptime(CONFIG['window_start_et'], '%Y-%m-%d %I:%M %p')
        )
        end_et = et.localize(
            datetime.strptime(CONFIG['window_end_et'], '%Y-%m-%d %I:%M %p')
        )

        # è½¬æ¢ä¸º UTC
        start_utc = start_et.astimezone(utc)
        end_utc = end_et.astimezone(utc)

        # è®¡ç®—çª—å£æ€»æ—¶é•¿
        total_hours = (end_utc - start_utc).total_seconds() / 3600

        return {
            'start_et': start_et,
            'end_et': end_et,
            'start_utc': start_utc,
            'end_utc': end_utc,
            'total_hours': total_hours,
        }

    def scrape_xtracker(self) -> dict:
        """ä» XTracker æŠ“å–å½“å‰æ¨æ–‡è®¡æ•°"""
        print(f"\n{'='*60}")
        print(f"[{datetime.now().strftime('%H:%M:%S')}] å¼€å§‹æŠ“å– XTracker...")
        print(f"URL: {self.config['xtracker_url']}")

        try:
            # å‘é€è¯·æ±‚
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            response = requests.get(
                self.config['xtracker_url'],
                headers=headers,
                timeout=30
            )
            response.raise_for_status()

            # è§£æ HTML
            soup = BeautifulSoup(response.text, 'html.parser')

            # å°è¯•å¤šä¸ªé€‰æ‹©å™¨æŸ¥æ‰¾è®¡æ•°å™¨
            count = None
            selectors = [
                '[data-testid="post-counter"]',
                '.post-count',
                '[class*="PostCounter"]',
                '[class*="tweet-count"]',
            ]

            for selector in selectors:
                element = soup.select_one(selector)
                if element:
                    text = element.get_text(strip=True)
                    # æå–æ•°å­—
                    import re
                    match = re.search(r'\d+', text)
                    if match:
                        count = int(match.group())
                        print(f"âœ“ æˆåŠŸæå–è®¡æ•°: {count} (é€‰æ‹©å™¨: {selector})")
                        break

            if count is None:
                # å¦‚æœæ‰€æœ‰é€‰æ‹©å™¨éƒ½å¤±è´¥ï¼Œå°è¯•æŸ¥æ‰¾ä»»ä½•åŒ…å«å¤§æ•°å­—çš„å…ƒç´ 
                print("âš  æœªæ‰¾åˆ°æ ‡å‡†é€‰æ‹©å™¨ï¼Œå°è¯•æ™ºèƒ½æœç´¢...")
                all_text = soup.get_text()
                numbers = re.findall(r'\d{3,}', all_text)  # æŸ¥æ‰¾3ä½ä»¥ä¸Šçš„æ•°å­—
                if numbers:
                    count = int(numbers[0])
                    print(f"âœ“ æ™ºèƒ½æœç´¢æ‰¾åˆ°è®¡æ•°: {count}")
                else:
                    raise ValueError("æ— æ³•ä»é¡µé¢æå–æ¨æ–‡è®¡æ•°")

            return {
                'current_count': count,
                'timestamp': datetime.now(pytz.UTC).isoformat(),
                'source': 'xtracker_scrape',
            }

        except Exception as e:
            print(f"âœ— æŠ“å–å¤±è´¥: {e}")
            raise

    def predict(self, current_count: int, elapsed_hours: float, remaining_hours: float) -> dict:
        """é¢„æµ‹æœ€ç»ˆæ¨æ–‡æ•°é‡"""
        print(f"\n{'â”€'*60}")
        print("ğŸ“Š é¢„æµ‹è®¡ç®—:")
        print(f"  å½“å‰æ¨æ–‡æ•°: {current_count}")
        print(f"  å·²è¿‡æ—¶é—´: {elapsed_hours:.1f} å°æ—¶")
        print(f"  å‰©ä½™æ—¶é—´: {remaining_hours:.1f} å°æ—¶")

        # è®¡ç®—å½“å‰é€Ÿç‡
        lambda_observed = current_count / elapsed_hours if elapsed_hours > 0 else 0
        lambda_prior = self.historical_data['avg_hourly_rate']

        # åŠ¨æ€æƒé‡ï¼ˆè¶Šæ¥è¿‘ç»“ç®—ï¼Œè¶Šä¿¡ä»»è§‚å¯Ÿæ•°æ®ï¼‰
        total_hours = elapsed_hours + remaining_hours
        progress_pct = (elapsed_hours / total_hours) * 100 if total_hours > 0 else 0
        w = min(1.0, progress_pct / 50)

        lambda_combined = w * lambda_observed + (1 - w) * lambda_prior

        print(f"\n  é€Ÿç‡ä¼°è®¡:")
        print(f"    è§‚å¯Ÿé€Ÿç‡: {lambda_observed:.2f} æ¡/å°æ—¶")
        print(f"    å…ˆéªŒé€Ÿç‡: {lambda_prior:.2f} æ¡/å°æ—¶")
        print(f"    èåˆé€Ÿç‡: {lambda_combined:.2f} æ¡/å°æ—¶ (æƒé‡ w={w:.2f})")

        # é¢„æµ‹æœªæ¥æ¨æ–‡æ•°
        lambda_future = lambda_combined * remaining_hours

        if self.config['model_type'] == 'poisson':
            future_dist = self._predict_poisson(lambda_future)
        else:
            future_dist = self._predict_neg_binom(lambda_future)

        # è®¡ç®—æ€»æ•°ç»Ÿè®¡
        expected_total = current_count + future_dist['mean']
        ci80_lower = current_count + future_dist['ci80_lower']
        ci80_upper = current_count + future_dist['ci80_upper']
        ci90_lower = current_count + future_dist['ci90_lower']
        ci90_upper = current_count + future_dist['ci90_upper']

        print(f"\n  é¢„æµ‹ç»“æœ:")
        print(f"    æœŸæœ›æ€»æ•°: {expected_total:.1f} æ¡")
        print(f"    80% ç½®ä¿¡åŒºé—´: [{ci80_lower:.0f}, {ci80_upper:.0f}]")
        print(f"    90% ç½®ä¿¡åŒºé—´: [{ci90_lower:.0f}, {ci90_upper:.0f}]")

        return {
            'expected_total': expected_total,
            'ci80_lower': ci80_lower,
            'ci80_upper': ci80_upper,
            'ci90_lower': ci90_lower,
            'ci90_upper': ci90_upper,
            'lambda_combined': lambda_combined,
            'progress_pct': progress_pct,
        }

    def _predict_poisson(self, lambda_total: float) -> dict:
        """Poisson åˆ†å¸ƒé¢„æµ‹"""
        dist = poisson(mu=lambda_total)
        return {
            'mean': dist.mean(),
            'variance': dist.var(),
            'ci80_lower': dist.ppf(0.1),
            'ci80_upper': dist.ppf(0.9),
            'ci90_lower': dist.ppf(0.05),
            'ci90_upper': dist.ppf(0.95),
        }

    def _predict_neg_binom(self, lambda_total: float) -> dict:
        """Negative Binomial åˆ†å¸ƒé¢„æµ‹"""
        alpha = 0.1  # è¿‡åº¦ç¦»æ•£å‚æ•°
        n = 1 / alpha
        p = 1 / (1 + alpha * lambda_total)

        dist = nbinom(n=n, p=p)
        return {
            'mean': dist.mean(),
            'variance': dist.var(),
            'ci80_lower': dist.ppf(0.1),
            'ci80_upper': dist.ppf(0.9),
            'ci90_lower': dist.ppf(0.05),
            'ci90_upper': dist.ppf(0.95),
        }

    def map_to_buckets(self, expected_total: float) -> dict:
        """å°†é¢„æµ‹æ˜ å°„åˆ° Polymarket åŒºé—´"""
        # ç®€åŒ–ç‰ˆï¼šç”Ÿæˆå¸¸è§åŒºé—´
        buckets = {}
        base = 160
        step = 20

        for i in range(22):  # 22 ä¸ªåŒºé—´
            if i < 21:
                bucket_name = f"{base + i*step}-{base + (i+1)*step - 1}"
            else:
                bucket_name = f"{base + i*step}+"

            # ç®€åŒ–æ¦‚ç‡è®¡ç®—ï¼šä½¿ç”¨æ­£æ€åˆ†å¸ƒè¿‘ä¼¼
            from scipy.stats import norm
            if i < 21:
                lower = base + i*step - expected_total
                upper = base + (i+1)*step - 1 - expected_total
                prob = norm.cdf(upper, scale=50) - norm.cdf(lower, scale=50)
            else:
                lower = base + i*step - expected_total
                prob = 1 - norm.cdf(lower, scale=50)

            buckets[bucket_name] = max(0, prob)

        # å½’ä¸€åŒ–
        total = sum(buckets.values())
        buckets = {k: v/total for k, v in buckets.items()}

        # æ˜¾ç¤ºå‰ 5 ä¸ªæœ€å¯èƒ½çš„åŒºé—´
        sorted_buckets = sorted(buckets.items(), key=lambda x: -x[1])[:5]
        print(f"\n  æœ€å¯èƒ½çš„åŒºé—´:")
        for bucket, prob in sorted_buckets:
            print(f"    {bucket:>10s}: {prob*100:5.2f}%")

        return buckets

    def save_prediction(self, prediction: dict):
        """ä¿å­˜é¢„æµ‹åˆ°æ–‡ä»¶"""
        self.predictions_history.append(prediction)

        # ä¿å­˜åˆ° JSON
        output_file = Path(self.config['output_file'])
        with open(output_file, 'w') as f:
            json.dump(self.predictions_history, f, indent=2, default=str)

        print(f"\nâœ“ é¢„æµ‹å·²ä¿å­˜åˆ°: {output_file}")

    def run_once(self) -> dict:
        """è¿è¡Œä¸€æ¬¡é¢„æµ‹"""
        # è§£ææ—¶é—´çª—å£
        window = self.parse_time_window()
        print(f"\næ—¶é—´çª—å£:")
        print(f"  å¼€å§‹ (ET): {window['start_et'].strftime('%Y-%m-%d %I:%M %p %Z')}")
        print(f"  ç»“æŸ (ET): {window['end_et'].strftime('%Y-%m-%d %I:%M %p %Z')}")
        print(f"  æ€»æ—¶é•¿: {window['total_hours']:.1f} å°æ—¶")

        # æŠ“å–å½“å‰è®¡æ•°
        snapshot = self.scrape_xtracker()

        # è®¡ç®—æ—¶é—´è¿›åº¦
        now = datetime.now(pytz.UTC)
        elapsed = (now - window['start_utc']).total_seconds() / 3600
        remaining = (window['end_utc'] - now).total_seconds() / 3600

        if remaining < 0:
            print(f"\nâš  å¸‚åœºå·²å…³é—­ï¼æœ€ç»ˆè®¡æ•°: {snapshot['current_count']}")
            return snapshot

        # é¢„æµ‹
        prediction_result = self.predict(
            snapshot['current_count'],
            elapsed,
            remaining
        )

        # æ˜ å°„åˆ°åŒºé—´
        buckets = self.map_to_buckets(prediction_result['expected_total'])

        # ç»„åˆç»“æœ
        result = {
            'timestamp': snapshot['timestamp'],
            'current_count': snapshot['current_count'],
            'elapsed_hours': elapsed,
            'remaining_hours': remaining,
            'progress_pct': prediction_result['progress_pct'],
            'prediction': prediction_result,
            'buckets': buckets,
        }

        # ä¿å­˜
        self.save_prediction(result)

        return result

    def run_continuous(self):
        """æŒç»­è¿è¡Œï¼ˆå®šæ—¶æŠ“å–ï¼‰"""
        print(f"\n{'='*60}")
        print("ğŸš€ Elon Musk Tweet é¢„æµ‹ç³»ç»Ÿå¯åŠ¨")
        print(f"æŠ“å–é—´éš”: {self.config['scrape_interval_seconds']} ç§’")
        print(f"æŒ‰ Ctrl+C åœæ­¢")
        print(f"{'='*60}")

        try:
            while True:
                try:
                    self.run_once()

                except Exception as e:
                    print(f"\nâœ— é”™è¯¯: {e}")
                    import traceback
                    traceback.print_exc()

                # ç­‰å¾…ä¸‹ä¸€æ¬¡æŠ“å–
                print(f"\nâ° ä¸‹æ¬¡æŠ“å–åœ¨ {self.config['scrape_interval_seconds']} ç§’å...")
                time.sleep(self.config['scrape_interval_seconds'])

        except KeyboardInterrupt:
            print(f"\n\nâ¹ ç”¨æˆ·åœæ­¢ï¼Œç¨‹åºé€€å‡º")


# ============================================================
# ä¸»ç¨‹åº
# ============================================================

def main():
    """ä¸»ç¨‹åº"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     Elon Musk Tweet Prediction System - MVP              â•‘
â•‘           Polymarket å¸‚åœºå®æ—¶é¢„æµ‹å·¥å…·                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # æ£€æŸ¥é…ç½®
    print("âš™ï¸  å½“å‰é…ç½®:")
    print(f"  å¸‚åœºURL: {CONFIG['market_url']}")
    print(f"  çª—å£å¼€å§‹: {CONFIG['window_start_et']} ET")
    print(f"  çª—å£ç»“æŸ: {CONFIG['window_end_et']} ET")
    print(f"  æŠ“å–é—´éš”: {CONFIG['scrape_interval_seconds']} ç§’")
    print(f"  æ¨¡å‹ç±»å‹: {CONFIG['model_type']}")

    # åˆ›å»ºé¢„æµ‹å™¨
    predictor = ElonTweetPredictor(CONFIG)

    # è¯¢é—®ç”¨æˆ·è¿è¡Œæ¨¡å¼
    print("\nè¯·é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("  1. å•æ¬¡è¿è¡Œï¼ˆæµ‹è¯•ï¼‰")
    print("  2. æŒç»­è¿è¡Œï¼ˆå®šæ—¶æŠ“å–ï¼‰")

    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1/2ï¼Œé»˜è®¤1): ").strip() or "1"

    if choice == "1":
        predictor.run_once()
    else:
        predictor.run_continuous()


if __name__ == "__main__":
    main()
