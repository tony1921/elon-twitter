#!/usr/bin/env python3
"""
è‡ªåŠ¨ä»XTrackerè·å–æ¨æ–‡æ•°é‡å¹¶è®°å½•
"""

import json
import os
import re
from datetime import datetime
from playwright.sync_api import sync_playwright

CONFIG = {
    'xtracker_url': 'https://xtracker.polymarket.com',
    'data_file': 'data/daily_tweets.json',
}


def load_data():
    if os.path.exists(CONFIG['data_file']):
        with open(CONFIG['data_file'], 'r', encoding='utf-8') as f:
            return json.load(f)
    return []


def save_data(data):
    os.makedirs("data", exist_ok=True)
    with open(CONFIG['data_file'], 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def add_record(count):
    data = load_data()
    today = datetime.now().strftime("%Y-%m-%d")
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # æ£€æŸ¥ä»Šå¤©æ˜¯å¦å·²æœ‰è®°å½•
    for record in data:
        if record['date'] == today:
            old_count = record['count']
            record['count'] = count
            record['updated_at'] = now

            # æ·»åŠ å†å²è®°å½•
            if 'history' not in record:
                record['history'] = []
            record['history'].append({
                'time': datetime.now().strftime("%H:%M"),
                'count': count
            })

            save_data(data)
            print(f"ğŸ“ æ›´æ–°: {old_count} â†’ {count} æ¡")
            return True

    # æ–°è®°å½•
    data.append({
        "date": today,
        "count": count,
        "created_at": now,
        "updated_at": now,
        "history": [{
            'time': datetime.now().strftime("%H:%M"),
            'count': count
        }]
    })
    save_data(data)
    print(f"ğŸ“ æ–°å¢: {today} - {count} æ¡")
    return True


def scrape_xtracker():
    """ä»XTrackerè·å–æ¨æ–‡æ•°é‡"""

    try:
        with sync_playwright() as p:
            print(f"\n[{datetime.now().strftime('%H:%M:%S')}] æ­£åœ¨è·å– XTracker æ•°æ®...")

            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            page.goto(CONFIG['xtracker_url'], timeout=30000)

            # ç­‰å¾…é¡µé¢åŠ è½½
            page.wait_for_timeout(3000)

            # è·å–é¡µé¢æ–‡æœ¬
            text = page.inner_text('body')

            # æŸ¥æ‰¾æ‰€æœ‰æ•°å­—
            numbers = re.findall(r'\b\d+\b', text)
            int_numbers = [int(n) for n in numbers]

            # è¿‡æ»¤ï¼šæ¨æ–‡æ•°é‡é€šå¸¸æ˜¯å‡ ååˆ°å‡ ç™¾ä¹‹é—´çš„æ•°å­—
            # æ’é™¤å¹´ä»½(2026)ã€å¤§æ•°å­—(>1000)
            candidates = [n for n in int_numbers if 50 <= n <= 500 and n != 2026]

            print(f"  æ‰¾åˆ°çš„å€™é€‰æ•°å­—: {candidates}")

            if candidates:
                # å¦‚æœæœ‰å¤šä¸ªå€™é€‰ï¼Œå–æœ€å¤§çš„ï¼ˆå¯èƒ½æ˜¯æ€»æ•°ï¼‰
                count = max(candidates)
                print(f"  âœ… æ¨æµ‹æ¨æ–‡æ•°: {count}")
                browser.close()
                return count

            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾ç‰¹å®šå…ƒç´ 
            print("  å°è¯•æŸ¥æ‰¾ç‰¹å®šå…ƒç´ ...")

            # å°è¯•å„ç§é€‰æ‹©å™¨
            selectors = [
                'text="Posts"',
                '[class*="post"]',
                '[class*="count"]',
                'h1', 'h2', 'h3',
            ]

            for selector in selectors:
                try:
                    elements = page.query_selector_all(selector)
                    for el in elements:
                        el_text = el.inner_text()
                        match = re.search(r'\b(\d{2,4})\b', el_text)
                        if match:
                            num = int(match.group(1))
                            if 50 <= num <= 500:
                                print(f"  âœ… ä»å…ƒç´ æ‰¾åˆ°: {num} ({selector})")
                                browser.close()
                                return num
                except:
                    pass

            browser.close()
            print("  âŒ æ— æ³•ç¡®å®šæ¨æ–‡æ•°é‡")
            return None

    except Exception as e:
        print(f"  âŒ é”™è¯¯: {e}")
        return None


def main():
    print("=" * 70)
    print("  ğŸ¤– è‡ªåŠ¨è·å–å¹¶è®°å½•æ¨æ–‡æ•°é‡")
    print("=" * 70)

    count = scrape_xtracker()

    if count:
        add_record(count)

        # æ˜¾ç¤ºæœ€è¿‘è®°å½•
        data = load_data()
        if data:
            print(f"\nğŸ“Š æœ€è¿‘è®°å½•:")
            for record in sorted(data, key=lambda x: x['date'], reverse=True)[:7]:
                print(f"  {record['date']}: {record['count']} æ¡")
    else:
        print("\nâŒ è·å–å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•æˆ–æ‰‹åŠ¨è®°å½•")

    print("=" * 70)


if __name__ == "__main__":
    main()
